{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ-VfNtOyJbsaxu43Kztf_cv1mgBG6ZIQZEVw&usqp=CAU'>\n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Taller #3: Web Scraping\n",
    "`Fecha de entrega: Marzo 11, 2021. (Antes del inicio de la próxima clase).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 1:\n",
    "\n",
    "- `[15 pts]` Hacer Web Scraping de 10 animales en Wikipedia (en búcle)\n",
    "- `[10 pts]` Obtener el **encabezado** de cada animal\n",
    "- `[15 pts]` Obtener todos los **textos** que están en las etiquetas de negrilla del primer párrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlace1 = \"https://es.wikipedia.org/wiki/Felis_silvestris_catus\"\n",
    "enlace2 =\"https://es.wikipedia.org/wiki/Canis_familiaris\"\n",
    "enlace3 =\"https://es.wikipedia.org/wiki/Delphinidae\"\n",
    "enlace4 =\"https://es.wikipedia.org/wiki/Elephantidae\"\n",
    "enlace5 =\"https://es.wikipedia.org/wiki/Crocodilia\"\n",
    "enlace6 =\"https://es.wikipedia.org/wiki/Panthera_leo\"\n",
    "enlace7 =\"https://es.wikipedia.org/wiki/Panthera_pardus\"\n",
    "enlace8 =\"https://es.wikipedia.org/wiki/Panthera_tigris\"\n",
    "enlace9 =\"https://es.wikipedia.org/wiki/Giraffa_camelopardalis\"\n",
    "enlace10 =\"https://es.wikipedia.org/wiki/Hippopotamidae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://es.wikipedia.org/wiki/Felis_silvestris_catus',\n",
       " 'https://es.wikipedia.org/wiki/Canis_familiaris',\n",
       " 'https://es.wikipedia.org/wiki/Delphinidae',\n",
       " 'https://es.wikipedia.org/wiki/Elephantidae',\n",
       " 'https://es.wikipedia.org/wiki/Crocodilia',\n",
       " 'https://es.wikipedia.org/wiki/Panthera_leo',\n",
       " 'https://es.wikipedia.org/wiki/Panthera_pardus',\n",
       " 'https://es.wikipedia.org/wiki/Panthera_tigris',\n",
       " 'https://es.wikipedia.org/wiki/Giraffa_camelopardalis',\n",
       " 'https://es.wikipedia.org/wiki/Hippopotamidae']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = [enlace1,enlace2,enlace3,enlace4,enlace5,enlace6,enlace7,enlace8,enlace9,enlace10]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Felis silvestris catus\n",
      "Canis familiaris\n",
      "Delphinidae\n",
      "Elephantidae\n",
      "Crocodilia\n",
      "Panthera leo\n",
      "Panthera pardus\n",
      "Panthera tigris\n",
      "Giraffa camelopardalis\n",
      "Hippopotamidae\n"
     ]
    }
   ],
   "source": [
    "for i in link:\n",
    "    dato = urllib.request.Request(i, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(dato)\n",
    "    sourse = webpage.read()\n",
    "    webpage.close()\n",
    "    soup = bs.BeautifulSoup(sourse,'html.parser')\n",
    "    titulo = soup.find(class_=\"firstHeading\").text\n",
    "    \n",
    "    print(titulo)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato doméstico', ' Felis silvestris catus', ' gato', ' minino', ' michino', ' michi', ' micho', ' mizo', ' miz', ' morroño', ' morrongo']\n",
      "['perro', ' Canis familiaris', ' Canis lupus familiaris', ' perro doméstico', ' can', ' chucho', ' tuso', ' choco']\n",
      "['delfines', ' Delphinidae', ' delfines oceánicos']\n",
      "['']\n",
      "['']\n",
      "['león', ' Panthera leo']\n",
      "['']\n",
      "['tigre', ' Panthera tigris']\n",
      "['']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "for i in link:\n",
    "    dato = urllib.request.Request(i, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(dato)\n",
    "    sourse = webpage.read()\n",
    "    webpage.close()\n",
    "    soup = bs.BeautifulSoup(sourse,'html.parser')\n",
    "    negrilla = soup.find(class_=\"mw-parser-output\").find_all(\"p\")\n",
    "    a = negrilla[0].find_all(\"b\")\n",
    "    b = str(a)\n",
    "    c = b.replace(\"<b>\", \"\").replace(\"</b>\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    d = c.split(',')\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2:\n",
    "- `[10 pts]` Usando regex, reemplazar todas las caracteres especiales del punto anterior (palabras en negrilla) por un asterisco (¡Ojo, los espacios se quedan!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato*dom*stico\n",
      "*Felis*silvestris*catus\n",
      "*gato\n",
      "*minino\n",
      "*michino\n",
      "*michi\n",
      "*micho\n",
      "*mizo\n",
      "*miz\n",
      "*morro*o\n",
      "*morrongo\n",
      "perro\n",
      "*Canis*familiaris\n",
      "*Canis*lupus*familiaris\n",
      "*perro*dom*stico\n",
      "*can\n",
      "*chucho\n",
      "*tuso\n",
      "*choco\n",
      "delfines\n",
      "*Delphinidae\n",
      "*delfines*oce*nicos\n",
      "\n",
      "\n",
      "le*n\n",
      "*Panthera*leo\n",
      "\n",
      "tigre\n",
      "*Panthera*tigris\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in link:\n",
    "    dato = urllib.request.Request(i, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(dato)\n",
    "    sourse = webpage.read()\n",
    "    webpage.close()\n",
    "    soup = bs.BeautifulSoup(sourse,'html.parser')\n",
    "    negrilla = soup.find(class_=\"mw-parser-output\").find_all(\"p\")\n",
    "    a = negrilla[0].find_all(\"b\")\n",
    "    b = str(a)\n",
    "    c = b.replace(\"<b>\", \"\").replace(\"</b>\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    d = c.split(',')\n",
    "    for p in range(0,len(d)):\n",
    "        print(re.sub(r\"[^a-zA-Z]\",\"*\",d[p]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
